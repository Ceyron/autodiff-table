<!DOCTYPE html>
<html lang="en">
<head>
<title>Automatic Differentiation Primitive Rules</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
</head>
<body class="p-2">
<h1 class="text-center">Explicit Autodiff Primitive Rules</h1>
<p class="text-center">
  Created with ‚ù§Ô∏è by <a href="https://www.youtube.com/@MachineLearningSimulation">Machine Learning & Simulation</a>.
</p>
<p class="text-center">
  <a href="https://twitter.com/felix_m_koehler?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @felix_m_koehler</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</p>
<p class="text-center">
  <small>Click the üîó to see the corresponding derivation video. üëâ <a href="https://www.youtube.com/watch?v=PwSaD50jTv8&list=PLISXH-iEM4Jn3SEi07q8MJmDD6BaMWlJE">Full Playlist</a>.</small>
</p>

<table class="table table-striped table-hover">
  <thead>
    <tr>
      <th>Primitive</th>
      <th>Primal</th>
      <th>Pushforward/Jvp</th>
      <th>Pullback/vJp</th>
    </tr>
  <tbody>
  <tr>
    <td><b>Explicit Scalar Rules</b></td>
    <td colspan="3"></td>
  </tr>
  <tr>
    <!-- Scalar Addition -->
    <td>Scalar Addition</td>
    <td>$z=x+y$</td>
    <td>
      $\dot{z}=\dot{x} + \dot{y}$
      <small><a href="https://youtu.be/PwSaD50jTv8">üîó</a></small>
    </td>
    <td>
      $
        \begin{align}
        \bar{x} &= \bar{z}
        \\
        \bar{y} &= \bar{z}
        \end{align}
      $
      <small><a href="https://youtu.be/SY2ga4ylwVM">üîó</a></small>
    </td>
  </tr>
  <tr>
    <!-- Scalar Multiplication -->
    <td>Scalar Multiplication</td>
    <td>$z=x \cdot y$</td>
    <td>
      $\dot{z}=y \cdot \dot{x} + x \cdot \dot{y}$
      <small><a href="https://youtu.be/kmJ9cIP_QPM">üîó</a></small>
    </td>
    <td>
      $
        \begin{align}
          \bar{x} &= \bar{z} \cdot y
          \\
          \bar{y} &= \bar{z} \cdot x
        \end{align}
      $
      <small><a href="https://youtu.be/ho8v1FpoaEg">üîó</a></small>
    </td>
  </tr>
  <tr>
    <!-- Scalar Negation-->
    <td>Scalar Negation</td>
    <td>$z=-x$</td>
    <td>$\dot{z} = - \dot{x}$</td>
    <td>$\bar{x} = - \bar{z}$</td>
  </tr>
  <tr>
    <!-- Scalar inversion-->
    <td>Scalar Inversion</td>
    <td>$z=\frac{1}{x}$</td>
    <td>$\dot{z} = - \frac{\dot{x}}{x^2}$</td>
    <td>$\bar{x} = - \frac{\bar{z}}{x^2}$</td>
  </tr>
  <tr>
    <!-- Scalar Power-->
    <td>Scalar Power</td>
    <td>$z = x^l$</td>
    <td>$\dot{z} = l \, x^{l-1} \, \dot{x}$</td>
    <td>$\bar{x} = \bar{z} \, l \, x^{l-1}$</td>
  </tr>
  <tr>
    <!-- Scalar Unary Function-->
    <td>Scalar Unary Function</td>
    <td>$z=f(x)$</td>
    <td>
      $\dot{z} = \frac{\partial f}{\partial x} \dot{x}$
      <small><a href="https://youtu.be/7YSd2kKqaxk">üîó</a></small>
    </td>
    <td>
      $\bar{x} = \bar{z} \frac{\partial f}{\partial x}$
      <small><a href="https://youtu.be/Agr-ozXtsOU">üîó</a></small>
    </td>
  </tr>
  <tr>
    <!-- General Scalar Function-->
    <td>General Scalar Function</td>
    <td>
      $
        \left\{ z^{[i]} \right\}_{i=1}^n = f\left(\left\{ x^{[j]} \right\}_{j=1}^m\right)
      $
    </td>
    <td>
      $
        \dot{z}^{[i]} = \sum_{j=1}^m \frac{\partial f^{[i]}}{\partial x^{[j]}} \dot{x}^{[j]}
      $
    </td>
    <td>
      $
        \bar{x}^{[j]} = \sum_{i=1}^n \bar{z}^{[i]} \frac{\partial f^{[i]}}{\partial x^{[j]}}
      $
    </td>
  </tr>
  <tr>
    <td><b>Explicit Tensor Rules</b></td>
    <td colspan="3"></td>
  </tr>
  <tr>
    <!-- Matrix-Vector Product-->
    <td>Matrix-Vector Product</td>
    <td>
      $
        \mathbf{z} = \mathbf{A} \mathbf{x}
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \dot{\mathbf{A}} \mathbf{x} + \mathbf{A} \dot{\mathbf{x}}
      $
      <small><a href="https://youtu.be/scyKnZzPbuU">üîó</a></small>
    </td>
    <td>
      $
      \begin{align}
        \bar{\mathbf{x}} &= \mathbf{A}^T \bar{\mathbf{z}}
        \\
        \bar{\mathbf{A}} &= \bar{\mathbf{z}} \mathbf{x}^T
      \end{align}
      $
      <small><a href="https://youtu.be/lqIhocjJLUc">üîó</a></small>
    </td>
  </tr>
  <tr>
    <!-- Matrix-Matrix Product-->
    <td>Matrix-Matrix Product</td>
    <td>
      $
        \mathbf{C} = \mathbf{A} \mathbf{B}
      $
    </td>
    <td>
      $
        \dot{\mathbf{C}} = \dot{\mathbf{A}} \mathbf{B} + \mathbf{A} \dot{\mathbf{B}}
      $
      <small><a href="https://youtu.be/X_egm4vWZNE">üîó</a></small>
    </td>
    <td>
      $
      \begin{align}
        \bar{\mathbf{A}} &= \bar{\mathbf{C}} \mathbf{B}^T
        \\
        \bar{\mathbf{B}} &= \mathbf{A}^T \bar{\mathbf{C}}
      \end{align}
      $
      <small><a href="https://youtu.be/O5YealZxi68">üîó</a></small>
    </td>
  </tr>
  <tr>
    <!-- Scalar-Vector Product-->
    <td>Scalar-Vector Product</td>
    <td>
      $
        \mathbf{z} = \alpha \mathbf{x}
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \dot{\alpha} \mathbf{x} + \alpha \dot{\mathbf{x}}
      $
    </td>
    <td>
      $
      \begin{align}
        \bar{\mathbf{x}} &= \bar{\mathbf{z}} \alpha
        \\
        \bar{\alpha} &= \bar{\mathbf{z}}^T \mathbf{x}
      \end{align}
      $
    </td>
  </tr>
  <tr>
    <!-- Scalar-Matrix Product-->
    <td>Scalar-Matrix Product</td>
    <td>
      $
        \mathbf{C} = \alpha \mathbf{A}
      $
    </td>
    <td>
      $
        \dot{\mathbf{C}} = \dot{\alpha} \mathbf{A} + \alpha \dot{\mathbf{A}}
      $
    </td>
    <td>
      $
      \begin{align}
        \bar{\mathbf{A}} &= \bar{\mathbf{C}} \alpha
        \\
        \bar{\alpha} &= \bar{\mathbf{C}} : \mathbf{A}
      \end{align}
      $
    </td>
  </tr>
  <tr>
    <!-- Matrix Transposition-->
    <td>Matrix Transposition</td>
    <td>
      $
        \mathbf{C} = \mathbf{A}^T
      $
    </td>
    <td>
      $
        \dot{\mathbf{C}} = \dot{\mathbf{A}}^T
      $
    </td>
    <td>
      $
        \bar{\mathbf{A}} = \bar{\mathbf{C}}^T
      $
    </td>
  </tr>
  <tr>
    <!-- Vector inner product -->
    <td>Vector Inner Product</td>
    <td>
      $
        \alpha = \mathbf{x}^T \mathbf{y}
      $
    </td>
    <td>
      $
        \dot{\alpha} = \dot{\mathbf{x}}^T \mathbf{y} + \mathbf{x}^T \dot{\mathbf{y}}
      $
    </td>
    <td>
      $
      \begin{align}
        \bar{\mathbf{x}} &= \bar{\alpha} \mathbf{y}
        \\
        \bar{\mathbf{y}} &= \bar{\alpha} \mathbf{x}
      \end{align}
      $
    </td>
  </tr>
  <tr>
    <!-- Vector outer product -->
    <td>Vector Outer Product</td>
    <td>
      $
        \mathbf{C} = \mathbf{x} \mathbf{y}^T
      $
    </td>
    <td>
      $
        \dot{\mathbf{C}} = \dot{\mathbf{x}} \mathbf{y}^T + \mathbf{x} \dot{\mathbf{y}}^T
      $
    </td>
    <td>
      $
      \begin{align}
        \bar{\mathbf{x}} &= \bar{\mathbf{C}} \mathbf{y}
        \\
        \bar{\mathbf{y}} &= \bar{\mathbf{C}}^T \mathbf{x}
      \end{align}
      $
    </td>
  </tr>
  <tr>
    <!-- Elementwise multiplication -->
    <td>Elementwise Multiplication</td>
    <td>
      $
        \mathbf{C} = \mathbf{A} \odot \mathbf{B}
      $
    </td>
    <td>
      $
        \dot{\mathbf{C}} = \dot{\mathbf{A}} \odot \mathbf{B} + \mathbf{A} \odot \dot{\mathbf{B}}
      $
    </td>
    <td>
      $
      \begin{align}
        \bar{\mathbf{A}} &= \bar{\mathbf{C}} \odot \mathbf{B}
        \\
        \bar{\mathbf{B}} &= \bar{\mathbf{C}} \odot \mathbf{A}
      \end{align}
      $
    </td>
  </tr>
  <tr>
    <!-- l2 Loss-->
    <td>l2 Loss (unscaled)</td>
    <td>
      $
        l = \frac{1}{2}||\mathbf{x}||_2^2
      $
    </td>
    <td>
      $
        \dot{l} = \mathbf{x}^T \dot{\mathbf{x}}
      $
      <small><a href="https://youtu.be/plb8mR-bcg8">üîó</a></small>
    </td>
    <td>
      $
        \bar{\mathbf{x}} = \, \bar{l} \, \mathbf{x}
      $
      <small><a href="https://youtu.be/TonUAqYCWAY">üîó</a></small>
    </td>
  </tr>
  <tr>
    <!-- L2 loss (MSE) -->
    <td>L2 Loss (MSE)</td>
    <td>
      $
        l = \frac{1}{2N}||\mathbf{x}||_2^2
      $
    </td>
    <td>
      $
        \dot{l} = \frac{1}{N}\mathbf{x}^T \dot{\mathbf{x}}
      $
    </td>
    <td>
      $
        \bar{\mathbf{x}} = \, \frac{\bar{l}}{N} \, \mathbf{x}
      $
    </td>
  </tr>
  <tr>
    <!-- Scalar Function Broadcasting-->
    <td>Scalar Function Broadcasting</td>
    <td>
      $
        \mathbf{z} = \sigma.( \mathbf{x} )
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \sigma'.( \mathbf{x} ) \odot \dot{\mathbf{x}}
      $
      <small><a href="https://youtu.be/S5cH4d42k4w">üîó</a></small>
    </td>
    <td>
      $
        \bar{\mathbf{x}} = \bar{\mathbf{z}} \odot \sigma'.( \mathbf{x} )
      $
      <small><a href="https://youtu.be/bLE6xsVSTUs">üîó</a></small>
    </td>
  </tr>
  <tr>
    <!-- Softmax-->
    <td>Softmax</td>
    <td>
      $\begin{align}
        \mathbf{z} &= \frac{\exp .(\mathbf{x})}{\mathbf{1}^T \exp . (\mathbf{x})} 
        \\
        z_i &= \frac{\exp(x_i)}{\sum_j \exp(x_j)}
      \end{align}$
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \mathbf{z} \odot \dot{\mathbf{x}} - \mathbf{z} (\mathbf{z}^T \dot{\mathbf{x}})
      $
      <small><a href="https://youtu.be/J7hK1Ba20yA">üîó</a></small>
    </td>
    <td>
      $
        \bar{\mathbf{x}} = \bar{\mathbf{z}} \odot \mathbf{z} - \mathbf{z} (\bar{\mathbf{z}}^T \mathbf{z})
      $
      <small><a href="https://youtu.be/46GOMwvjayc">üîó</a></small>
    </td>
  </tr>
  <tr>
    <!-- FFT-->
    <td>FFT</td>
    <td>
      $
        \mathbf{z} = \text{fft}( \mathbf{x} )
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \text{fft}( \dot{\mathbf{x}} )
      $
    </td>
    <td>
      $
        \begin{align}
          \text{JAX} \quad \bar{\mathbf{x}} &= \text{fft}( \bar{\mathbf{z}} )
          \\
          \text{PyTorch} \quad \bar{\mathbf{x}} &= \text{fft}( \bar{\mathbf{z}}^* )
        \end{align}
      $
      <br>
      <a href="https://github.com/google/jax/issues/4891">Details on Differences</a>
    </td>
  </tr>
  <tr>
    <!-- IFFT-->
    <td>IFFT</td>
    <td>
      $
        \mathbf{z} = \text{ifft}( \mathbf{x} )
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \text{ifft}( \dot{\mathbf{x}} )
      $
    </td>
    <td>
      $
        \begin{align}
          \text{JAX} \quad \bar{\mathbf{x}} &= \text{ifft}( \bar{\mathbf{z}} )
          \\
          \text{PyTorch} \quad \bar{\mathbf{x}} &= \text{ifft}( \bar{\mathbf{z}}^* )
        \end{align}
      $
      <br>
    <td>
  </tr>
  <tr>
    <!-- RFFT -->
    <td>RFFT</td>
    <td>
      $
        \mathbf{z} = \text{rfft}( \mathbf{x} )
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \text{rfft}( \dot{\mathbf{x}} )
      $
    </td>
    <td>
      JAX
      <br>
      $
        \begin{align}
          \bar{\mathbf{x}} &= \mathcal{R}(\text{fft}( \text{pad}(\bar{\mathbf{z}}, (0, N//2 - 1) )))
        \end{align}
      $
    </td>
  </tr>
  <!-- IRFFT -->
  <tr>
    <td>IRFFT</td>
    <td>
      $
        \mathbf{z} = \text{irfft}( \mathbf{x} )
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \text{irfft}( \dot{\mathbf{x}} )
      $
    </td>
    <td>
      JAX
      <br>
      $
        \begin{align}
          <!-- \bar{\mathbf{x}} &= \mathcal{R}(\text{ifft}( \text{pad}(\bar{\mathbf{z}}, (0, N//2 - 1) ))) -->
           t &= \text{rfft}(\bar{z})
           \\
           t_{1:(N//2)} &= 2 \cdot t_{1:(N//2)}
           \\
           \bar{x} &= \frac{1}{N} t
        \end{align}
      $
    </td>
  </tr>
  <tr>
    <!-- Convolution with Arbitrary Zero Padding -->
    <td>
      Convolution with
      <br>
      Arbitrary Zero Padding
      <br>
      Convolution in the
      <br>
      sense of CNNs
      <br>
      (actually cross-correlation)
      <br>
      <a href="https://fkoehler.site/conv-autodiff-table-reworked/">More Details</a>
    </td>
    <td>
<pre>z = conv(
  x, w,
  padding=(p_l, p_r),
)</pre>
    </td>
    <td>
<pre>dot_z = conv(
  dot_x, w,
  padding=(p_l, p_r),
) + conv(
  x, dot_w,
  padding=(p_l, p_r),
)</pre>
    </td>
    <td>
<pre>x_cot = conv(
  z_cot, w,
  padding=(K-1-p_l, K-1-p_r),
  flip_kernel=True,
  permute_kernel=True,
)</pre>
<pre>w_cot = conv(
  x, z_cot,
  padding=(p_l, p_r),
  permute_kernel=True,
  permute_input=True,
)</pre>
    </td>
  </tr>
  <tr>
    <!-- Vector Unary Function-->
    <td>Vector Unary Function</td>
    <td>
      $
        \mathbf{z} = f(\mathbf{x})
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \frac{\partial f}{\partial \mathbf{x}} \dot{\mathbf{x}}
      $
    </td>
    <td>
      $
      \begin{align}
        \bar{\mathbf{x}}^T &= \bar{\mathbf{z}}^T \frac{\partial f}{\partial \mathbf{x}}
        \\
        &\text{or}
        \\
        \bar{\mathbf{x}} &= \left( \frac{\partial f}{\partial \mathbf{x}} \right)^T \bar{\mathbf{z}}
      \end{align}
      $
    </td>
  </tr>
  <tr>
    <!-- General Vector Function-->
    <td>General Vector Function</td>
    <td>
      $
        \left\{ \mathbf{z}^{[i]}\right\}_{i=1}^n = f(\left\{ \mathbf{x}^{[j]}\right\}_{j=1}^m)
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}}^{[i]} = \sum_{j=1}^m \frac{\partial f^{[i]}}{\partial \mathbf{x}^{[j]}} \dot{\mathbf{x}}^{[j]}
      $
    </td>
    <td>
      $
        \bar{\mathbf{x}}^{[j],T} = \sum_{i=1}^n \bar{\mathbf{z}}^{[i],T} \frac{\partial f^{[i]}}{\partial \mathbf{x}^{[j]}}
      $
    </td>
  </tr>
  <tr>
    <td><b>Array Manipulations</b></td>
    <td colspan="3"></td>
  </tr>
  <tr>
    <!-- Concatenation-->
    <td>Concatenation</td>
    <td>
      $
        \mathbf{z} = \text{concat}(\mathbf{x}, \mathbf{y})
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \text{concat}(\dot{\mathbf{x}}, \dot{\mathbf{y}})
      $
    </td>
    <td>
      $
        \begin{align}
          \bar{\mathbf{x}} &= \text{slice}(\bar{\mathbf{z}}, 0, \text{len}(\mathbf{x}))
          \\
          \bar{\mathbf{y}} &= \text{slice}(\bar{\mathbf{z}}, \text{len}(\mathbf{x}), \text{len}(\mathbf{z}))
        \end{align}
      $
    </td>
  </tr>
  <tr>
    <!-- Slicing-->
    <td>Slicing</td>
    <td>
      $
        \mathbf{z} = \text{slice}(\mathbf{x}, a, b)
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \text{slice}(\dot{\mathbf{x}}, a, b)
      $
    </td>
    <td>
      $
        \begin{align}
          \bar{\mathbf{x}} &= \text{concat}(\text{zeros}(a), \bar{\mathbf{z}}, \text{zeros}(\text{len}(\mathbf{x}) - b))
          \\
          &\text{or}
          \\
          \bar{\mathbf{x}} &= \text{pad}(\bar{\mathbf{z}}, (a, \text{len}(\mathbf{x}) - b))
        \end{align}
      $
    </td>
  </tr>
  <tr>
    <!-- Padding-->
    <td>(Zero) Padding</td>
    <td>
      $
        \mathbf{z} = \text{pad}(\mathbf{x}, (a, b))
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \text{pad}(\dot{\mathbf{x}}, (a, b))
      $
    </td>
    <td>
      $
        \bar{\mathbf{x}} = \text{slice}(\bar{\mathbf{z}}, a, \text{len}(\mathbf{z}) - b)
      $
    </td>
  </tr>
  <tr>
    <!-- Reshaping-->
    <td>Reshaping</td>
    <td>
      $
        \mathbf{z} = \text{reshape}(\mathbf{x}, \text{shape}(\mathbf{z}))
      $
    </td>
    <td>
      $
        \dot{\mathbf{z}} = \text{reshape}(\dot{\mathbf{x}}, \text{shape}(\mathbf{z}))
      $
    </td>
    <td>
      $
        \bar{\mathbf{x}} = \text{reshape}(\bar{\mathbf{z}}, \text{shape}(\mathbf{x}))
      $
    </td>
  </tr>
  </tbody>

</table> 


<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" integrity="sha384-w76AqPfDkMBDXo30jS1Sgez6pr3x5MlQ1ZAGC+nuZB+EYdgRZgiwxhTBTkF7CXvN" crossorigin="anonymous"></script>
</body>
</html>
